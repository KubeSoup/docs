{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4f0017-524a-4d1c-b360-c6677011ddc0",
   "metadata": {},
   "source": [
    "## SparkNLP tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3e696-fe91-4226-942f-65b82c23b193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the sparknlp package and make a tar file to distribute the package across workers\n",
    "!conda install -y -c johnsnowlabs spark-nlp==3.4.3\n",
    "!conda pack -f -o base_conda_env.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a09839b-edb1-435b-bacc-18a7a32ae840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:23:57.149760Z",
     "iopub.status.busy": "2022-05-02T09:23:57.149414Z",
     "iopub.status.idle": "2022-05-02T09:24:38.953590Z",
     "shell.execute_reply": "2022-05-02T09:24:38.953156Z",
     "shell.execute_reply.started": "2022-05-02T09:23:57.149735Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.johnsnowlabs.nlp#spark-nlp-spark32_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-70cfd393-0564-4c39-a02e-2d3ebc7d5fb9;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp-spark32_2.12;3.4.3 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.7.0-M11 in central\n",
      "\tfound joda-time#joda-time;2.10.10 in central\n",
      "\tfound org.joda#joda-convert;2.2.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 1102ms :: artifacts dl 152ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp-spark32_2.12;3.4.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.10 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.joda#joda-convert;2.2.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.7.0-M11 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 by [com.amazonaws#aws-java-sdk-bundle;1.11.901] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   27  |   0   |   0   |   1   ||   26  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-70cfd393-0564-4c39-a02e-2d3ebc7d5fb9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 26 already retrieved (0kB/11ms)\n",
      "22/05/02 09:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 51882)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# add the maven packages you want to use\n",
    "spark_packages = [\n",
    "    \"io.delta:delta-core_2.12:1.1.0\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.1\",\n",
    "    \"com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.3\", # for sparknlp usage\n",
    "]\n",
    "spark_packages = \",\".join(spark_packages)\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default-java\" # comment for java8\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages \"{spark_packages}\" pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "namespace = os.environ[\"NAMESPACE\"] # usually \"firstname-lastname\"\n",
    "notebook_name = os.environ[\"NOTEBOOK_NAME\"] # might be helpful\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(f\"{namespace}-spark-app\")\n",
    "    .config(\"spark.archives\", \"base_conda_env.tar.gz#environment\") # required when you want to use your installed packages on spark workers\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.WebIdentityTokenCredentialsProvider\") # Either use built in authentication for S3\n",
    "    # The section with `spark.kubernetes.executor.volumes.persistentVolumeClaim` is for\n",
    "    # specifying the usage of a local volume to enable more storage space for Disk Spilling\n",
    "    # If not need, just completely remove the properties\n",
    "    # you need only to modify the necessary size for the volume under `sizeLimit`\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName\", \"OnDemand\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass\", \"efs-csi\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit\", \"100Gi\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path\", \"/data\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly\", \"false\") # disk storage for spilling\n",
    "    # The section with `spark.kubernetes.node.selector` is for specifying\n",
    "    # what nodes to use for the executor and in which Availability Zone (AZ)\n",
    "    # They need to be in the same zone\n",
    "    # .config(\"spark.kubernetes.node.selector.topology.ebs.csi.aws.com/zone\", \"eu-central-1a\") # node selector\n",
    "    # .config(\"spark.kubernetes.node.selector.plural.sh/scalingGroup\", \"xlarge-mem-optimized-on-demand\") # node selector, read \"Node Groups for the Spark Executors\"\n",
    "    .config(\"spark.executor.instances\", \"2\") # number of Executors\n",
    "    .config(\"spark.executor.memory\", \"6g\") # Executor memory\n",
    "    .config(\"spark.executor.cores\", \"1\") # Executor cores\n",
    "    # .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.3\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963d7f8c-79ef-40c2-8508-2143cf22aaad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:24:38.954620Z",
     "iopub.status.busy": "2022-05-02T09:24:38.954488Z",
     "iopub.status.idle": "2022-05-02T09:24:39.144376Z",
     "shell.execute_reply": "2022-05-02T09:24:39.144057Z",
     "shell.execute_reply.started": "2022-05-02T09:24:38.954603Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.2.1', '3.4.3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark.version, sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc19b0f-3b89-45e8-b4bb-3a4b8aca8a24",
   "metadata": {},
   "source": [
    "## Using SparkNLP Models\n",
    "\n",
    "In distributed settings, SparkNLP only supports loading files from distributed file systems like (hdfs, s3, etc). \n",
    "\n",
    "So, we have to download the models from the [SparkNLP ModelHub](https://nlp.johnsnowlabs.com/models) and add it to the `s3://opengptx/sparknlp-models` folder.\n",
    "\n",
    "Another point to note, since we are using the latest version of the Spark, it might be some of the models are not supported with our deployment (like models compatible with Spark 2.x). \n",
    "\n",
    "If you select the [Models for SparkNLP version >= 3.4.x](https://nlp.johnsnowlabs.com/models?edition=Spark+NLP+3.4), the chances for being compatible is very high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93b8b3-8ddf-4a56-bd2c-cb56ae0a64c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline, PipelineModel\n",
    "\n",
    "pipeline_dl = PipelineModel.load(\"s3a://opengptx/sparknlp-models/onto_recognize_entities_electra_small_en_3.0.0_3.0_1616444187316\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746d0caa-4fc1-4829-a7c7-53b399d7986c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:25:00.596834Z",
     "iopub.status.busy": "2022-05-02T09:25:00.596719Z",
     "iopub.status.idle": "2022-05-02T09:25:00.599055Z",
     "shell.execute_reply": "2022-05-02T09:25:00.598750Z",
     "shell.execute_reply.started": "2022-05-02T09:25:00.596820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  ['SparkNLP is from John Snow Labs!'],\n",
    "  ['Apple products are overpriced']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21bf4b87-db22-4bfd-9c9c-1a9b5d345a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:25:00.599765Z",
     "iopub.status.busy": "2022-05-02T09:25:00.599541Z",
     "iopub.status.idle": "2022-05-02T09:25:02.014162Z",
     "shell.execute_reply": "2022-05-02T09:25:02.013759Z",
     "shell.execute_reply.started": "2022-05-02T09:25:00.599750Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(sentences).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b621dab4-3197-4482-86bb-aaae5e90042e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:25:02.015220Z",
     "iopub.status.busy": "2022-05-02T09:25:02.014856Z",
     "iopub.status.idle": "2022-05-02T09:25:02.880668Z",
     "shell.execute_reply": "2022-05-02T09:25:02.880288Z",
     "shell.execute_reply.started": "2022-05-02T09:25:02.015203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_df = pipeline_dl.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e67f3f-9cd2-4e3b-b7f0-8a04b16c4c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:25:02.881622Z",
     "iopub.status.busy": "2022-05-02T09:25:02.881317Z",
     "iopub.status.idle": "2022-05-02T09:25:19.491930Z",
     "shell.execute_reply": "2022-05-02T09:25:19.491566Z",
     "shell.execute_reply.started": "2022-05-02T09:25:02.881607Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|          embeddings|                 ner|            entities|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|SparkNLP is from ...|[{document, 0, 31...|[{document, 0, 31...|[{token, 0, 7, Sp...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 7, Sp...|\n",
      "|Apple products ar...|[{document, 0, 28...|[{document, 0, 28...|[{token, 0, 4, Ap...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 4, Ap...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "annotations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a561a01f-6b73-4d9e-892e-37da86b29a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T09:25:19.492822Z",
     "iopub.status.busy": "2022-05-02T09:25:19.492520Z",
     "iopub.status.idle": "2022-05-02T09:25:20.228285Z",
     "shell.execute_reply": "2022-05-02T09:25:20.227979Z",
     "shell.execute_reply.started": "2022-05-02T09:25:19.492806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|entities                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 0, 7, SparkNLP, {entity -> ORG, sentence -> 0, chunk -> 0}, []}, {chunk, 17, 31, John Snow Labs!, {entity -> ORG, sentence -> 0, chunk -> 1}, []}]|\n",
      "|[{chunk, 0, 4, Apple, {entity -> ORG, sentence -> 0, chunk -> 0}, []}]                                                                                     |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotations_df.select(\"entities\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c39501-0787-4add-bf75-2caa88d2a83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
