{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2474ab-0bbd-4a2b-a9f3-d0e1f44c0ba8",
   "metadata": {},
   "source": [
    "## PySpark Example Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff55fbc-1193-4eb1-bb43-2f012d7965b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Downloading & Packaging python packaging for Spark drivers and workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361d7d5-1939-484d-a4e2-7391b64f884c",
   "metadata": {},
   "source": [
    "You can install all the packages you need and make it distributable using `conda pack`, so that it can be provided to all the spark workers.\n",
    "\n",
    "`conda pack` can take care of packaging both the conda and pip packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59eb9e7-7801-4080-8c69-76105298c9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!conda install -y pandas pyarrow==7.0.0 conda-pack\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!conda pack -f -o base_conda_env.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363b623-bf80-40c7-a736-2076e8ca8158",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b240cc-aeb3-4f6b-bbec-747f69de9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType, LongType, ArrayType\n",
    "import pyspark.sql.functions as F\n",
    "import spacy\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46244b4b-9969-4ae2-a8c5-2e9f417939fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maven_packages = [\n",
    "    \"io.delta:delta-core_2.12:1.1.0\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.1\",\n",
    "    # \"com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.3\", # for sparknlp\n",
    "]\n",
    "maven_packages = \",\".join(maven_packages)\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default-java\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages \"{maven_packages}\" pyspark-shell'\n",
    "# required when you want to use your installed packages on spark workers\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4f98c3-c33f-4ef6-b784-16ef88df9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = os.environ[\"NAMESPACE\"] # usually \"firstname-lastname\"\n",
    "notebook_name = os.environ[\"NOTEBOOK_NAME\"] # might be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f3a95d-1fec-43c2-8cfc-ca2a9d222737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e0af081-cce0-431b-9152-ec7e22b922fd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 172ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9e0af081-cce0-431b-9152-ec7e22b922fd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/5ms)\n",
      "22/04/06 02:52:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(f\"{namespace}-spark-app\")\n",
    "    .config(\"spark.archives\", \"base_conda_env.tar.gz#environment\") # required when you want to use your installed packages on spark workers\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.WebIdentityTokenCredentialsProvider\") # Either use built in authentication for S3\n",
    "    # or a custom one with specific S3 Access and Secret Keys\n",
    "    # .config(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_S3_ACCESS_KEY']) # optional\n",
    "    # .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_S3_SECRET_KEY']) # optional\n",
    "    # The section with `spark.kubernetes.executor.volumes.persistentVolumeClaim` is for\n",
    "    # specifying the usage of a loca volume to enable more storage space for Disk Spilling\n",
    "    # If not need, just completely remove the properties\n",
    "    # you need only to modify the necessary size for the volume under `sizeLimit`\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName\", \"OnDemand\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass\", \"ebs-csi\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit\", \"100Gi\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path\", \"/data\") # disk storage for spilling\n",
    "    # .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly\", \"false\") # disk storage for spilling\n",
    "    # The section with `spark.kubernetes.node.selector` is for specifying\n",
    "    # what nodes to use for the executor and in which Availability Zone (AZ)\n",
    "    # They need to be in the same zone\n",
    "    # .config(\"spark.kubernetes.node.selector.topology.ebs.csi.aws.com/zone\", \"eu-central-1a\") # node selector\n",
    "    # .config(\"spark.kubernetes.node.selector.plural.sh/scalingGroup\", \"large-mem-optimized-on-demand\") # node selector\n",
    "    .config(\"spark.executor.instances\", \"2\") # number of Executors\n",
    "    .config(\"spark.executor.memory\", \"12g\") # Executor memory\n",
    "    .config(\"spark.executor.cores\", \"1\") # Executor cores\n",
    "    .config(\"spark.executor.pyspark.memory\", \"8g\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f1d8f-87f7-4119-896b-508d2a7d95b8",
   "metadata": {},
   "source": [
    "Spark can infer schema at runtime but it's better to provide it as it can serve as a validation for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d98899b-0e0a-4224-a4e8-18fad1245afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655a150-30e4-499f-a4e9-7c64c93b962e",
   "metadata": {},
   "source": [
    "Doc - https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21365c6-a7cb-4eb3-b5a1-d851ac3f9bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 02:52:48 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").schema(schema).load(\"s3a://opengptx/dataset/en.noclean_parquet_snappy/c4-train.00000-of-07168.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8404e52-f4e3-43fd-a5d6-90ecdeb79aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b50431-7e7a-42f9-97e4-f0a811b045e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5773040-5eff-4638-b537-4d868d2c4c64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e6bab-039f-4e46-bb66-a78d07463583",
   "metadata": {},
   "source": [
    "There are 3 different ways to do operations on the dataframes/tables:\n",
    "1. Spark SQL Functions\n",
    "2. PySpark UDF (User Defined Function)\n",
    "3. Pandas UDF (a.k.a. Vectorized UDFs)\n",
    "\n",
    "We will cover examples for each type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833793b-7478-47a8-ba93-84b65c740661",
   "metadata": {},
   "source": [
    "#### Count number of words in text using Pandas UDF\n",
    "\n",
    "User Guide - https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#pandas-udfs-a-k-a-vectorized-udfs\n",
    "\n",
    "Docs - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.pandas_udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac7fbd4-80f9-4ef8-aeff-b88de5c7c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"long\")\n",
    "def count_words_series_pd_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.str.split(\" \").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "167448c5-fb24-44dc-b170-077c44806794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "|                text|          timestamp|                 url|word_count_pd_udf|\n",
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "|November 24, 2016...|2019-04-24 16:35:11|http://sevendayne...|              565|\n",
      "|Beginners BBQ Cla...|2019-04-25 12:57:54|https://klyq.com/...|              232|\n",
      "|Download Link pag...|2019-04-20 10:50:34|https://yyen.info...|              165|\n",
      "|Restore from larg...|2019-04-21 10:07:13|https://forums.ma...|             1786|\n",
      "|Jet Rashie Set - ...|2019-04-20 06:57:02|https://www.skyea...|              392|\n",
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"word_count_pd_udf\", count_words_series_pd_udf(F.col((\"text\")))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94a190-8fe2-43cb-8b15-0893b0884562",
   "metadata": {},
   "source": [
    "#### Count number of words using Spark SQL Functions\n",
    "\n",
    "Docs - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de250f7-d39a-4df0-8810-54653c565a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "|                text|          timestamp|                 url|word_count_native|\n",
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "|November 24, 2016...|2019-04-24 16:35:11|http://sevendayne...|              565|\n",
      "|Beginners BBQ Cla...|2019-04-25 12:57:54|https://klyq.com/...|              232|\n",
      "|Download Link pag...|2019-04-20 10:50:34|https://yyen.info...|              165|\n",
      "|Restore from larg...|2019-04-21 10:07:13|https://forums.ma...|             1786|\n",
      "|Jet Rashie Set - ...|2019-04-20 06:57:02|https://www.skyea...|              392|\n",
      "+--------------------+-------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"word_count_native\", F.size(F.split(F.col('text'), ' '))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9d0e5-1090-4e05-9e8f-d358d7e0d9f1",
   "metadata": {},
   "source": [
    "#### Remove stop words using PySpark UDF\n",
    "Docs - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c8a03f-637c-46d9-8d2e-ce9c9dcff092",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "en_stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a4bf90-ce41-4cd1-ad83-648c961cf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> List[str]:\n",
    "    lst=[]\n",
    "    for token in text.split():\n",
    "        if token.lower() not in en_stopwords:\n",
    "            lst.append(token)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ffe201-3359-45c1-905e-de25f7c42d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_udf = F.udf(lambda z: remove_stopwords(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e740bc-a069-44bf-adc2-435bdd5f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+----------------------+\n",
      "|                text|          timestamp|                 url|text_without_stopwords|\n",
      "+--------------------+-------------------+--------------------+----------------------+\n",
      "|November 24, 2016...|2019-04-24 16:35:11|http://sevendayne...|  [November, 24,, 2...|\n",
      "|Beginners BBQ Cla...|2019-04-25 12:57:54|https://klyq.com/...|  [Beginners, BBQ, ...|\n",
      "|Download Link pag...|2019-04-20 10:50:34|https://yyen.info...|  [Download, Link, ...|\n",
      "|Restore from larg...|2019-04-21 10:07:13|https://forums.ma...|  [Restore, larger,...|\n",
      "|Jet Rashie Set - ...|2019-04-20 06:57:02|https://www.skyea...|  [Jet, Rashie, Set...|\n",
      "+--------------------+-------------------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"text_without_stopwords\", remove_stopwords_udf(F.col(\"text\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3381491-6836-464f-8735-4c5a41d0efb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example\n",
    "Let's add UUID to our original table and create a metadata table, with count of words and stopwords in text.\n",
    "We will later join the metadata table with our original table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ddcaf-2562-4d7c-957f-769f3e0d0431",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Add UUID to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2753461-6959-4c11-b03d-209c46f98004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"uuid\", F.expr(\"uuid()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29876977-a1d2-4f16-b143-3c633e3f171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns\n",
    "df = df.select(\"uuid\", \"text\", \"timestamp\", \"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe76f29-b540-4bbf-99bf-d142e48f1428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|                uuid|                text|          timestamp|                 url|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|76819797-942a-405...|November 24, 2016...|2019-04-24 16:35:11|http://sevendayne...|\n",
      "|e8cf0316-8eba-404...|Beginners BBQ Cla...|2019-04-25 12:57:54|https://klyq.com/...|\n",
      "|6fa41fc7-779a-42c...|Download Link pag...|2019-04-20 10:50:34|https://yyen.info...|\n",
      "|089d39bd-4ce7-492...|Restore from larg...|2019-04-21 10:07:13|https://forums.ma...|\n",
      "|75a78088-ad0f-477...|Jet Rashie Set - ...|2019-04-20 06:57:02|https://www.skyea...|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75ced9-fbb2-4ff3-ab74-7cb5950c1684",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fabce7c1-1a7c-44e1-a190-7624ed6c04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=LongType())\n",
    "def count_stopwords(text: str) -> int:\n",
    "    return len(text.split()) - len(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dcc16cb-0946-46c0-8a40-28120686d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata table\n",
    "df_metadata = df.select(\"uuid\", count_words_series_pd_udf(df.text).alias(\"count_words\"), count_stopwords(df.text).alias(\"count_stopwords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9811eda0-e2ae-4c1e-bd73-a7a5682d9ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------------+\n",
      "|                uuid|count_words|count_stopwords|\n",
      "+--------------------+-----------+---------------+\n",
      "|76819797-942a-405...|        565|            197|\n",
      "|e8cf0316-8eba-404...|        232|             99|\n",
      "|6fa41fc7-779a-42c...|        165|             33|\n",
      "|089d39bd-4ce7-492...|       1786|            460|\n",
      "|75a78088-ad0f-477...|        392|            123|\n",
      "+--------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_metadata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43003442-1d05-4c12-852b-277b83abd5ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataframe Joins\n",
    "Docs - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90507286-ffac-4585-a6b7-46cf7b76b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df.join(df_metadata, on=\"uuid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3937686-bf4b-4154-9c00-14df2de7c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+-----------+---------------+\n",
      "|                uuid|                text|          timestamp|                 url|count_words|count_stopwords|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+---------------+\n",
      "|089d39bd-4ce7-492...|Restore from larg...|2019-04-21 10:07:13|https://forums.ma...|       1786|            460|\n",
      "|6fa41fc7-779a-42c...|Download Link pag...|2019-04-20 10:50:34|https://yyen.info...|        165|             33|\n",
      "|e8cf0316-8eba-404...|Beginners BBQ Cla...|2019-04-25 12:57:54|https://klyq.com/...|        232|             99|\n",
      "|38b3621e-cc99-4af...|Skip to Main Cont...|2019-04-23 14:13:51|https://www.expre...|        466|             93|\n",
      "|75a78088-ad0f-477...|Jet Rashie Set - ...|2019-04-20 06:57:02|https://www.skyea...|        392|            123|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Joins require reshuffling of data, thus the order mismatch\n",
    "df_full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314505b-7419-4161-afee-858016b1ceac",
   "metadata": {},
   "source": [
    "#### Saving Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98359afe-2557-45cc-ba96-c2f8ba821491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_full.write.format(\"parquet\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"s3a://opengptx/dataset/demo/c4-train.00000-of-07168_full.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa824040-be12-4a28-8756-a265a4f85c1d",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "Working with DataFrame - https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\n",
    "\n",
    "Pandas API on Spark - https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbd1a5-3ee3-444d-9969-9a08edb76759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
