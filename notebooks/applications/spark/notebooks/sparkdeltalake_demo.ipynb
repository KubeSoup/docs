{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edbc28-5078-4e8a-87d9-ea9a4281d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pandas pyarrow==7.0.0 conda-pack\n",
    "!pip install zstd\n",
    "!conda pack -f -o base_conda_env.tar.gz # create conda-pack with the environment and its packages for the executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb58f0c-d62c-4a85-a83b-8856b03fb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default-java\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.3.1\" pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "namespace = \"user-name\" # usually \"firstname-lastname\"\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(f\"{namespace}-spark-app\")\n",
    "    .master(\"k8s://https://kubernetes.default\")\n",
    "    .config(\"spark.kubernetes.namespace\", namespace)\n",
    "    .config(\"spark.archives\", \"base_conda_env.tar.gz#environment\") # pass the conda-pack with the necessary packages so the executors can load them\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.WebIdentityTokenCredentialsProvider\") # Either use built in authentication for S3\n",
    "    # or a custom one with specific S3 Access and Secret Keys below\n",
    "    # .config(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_S3_ACCESS_KEY']) # optional\n",
    "    # .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_S3_SECRET_KEY']) # optional\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"default-editor\")\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"public.ecr.aws/atcommons/spark/python:latest\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.port\", \"2222\")\n",
    "    .config(\"spark.driver.blockManager.port\", \"7078\")\n",
    "    .config(\"spark.blockManager.port\", \"7079\")\n",
    "    .config(\"spark.kubernetes.executor.annotation.traffic.sidecar.istio.io/excludeOutboundPorts\", \"7078\")\n",
    "    .config(\"spark.kubernetes.executor.annotation.traffic.sidecar.istio.io/excludeInboundPorts\", \"7079\")\n",
    "    # The section with `spark.kubernetes.executor.volumes.persistentVolumeClaim` is for\n",
    "    # specifying the usage of a loca volume to enable more storage space for Disk Spilling\n",
    "    # If not need, just completely remove the properties\n",
    "    # you need only to modify the necessary size for the volume under `sizeLimit`\n",
    "    .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName\", \"OnDemand\") # disk storage for spilling\n",
    "    .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass\", \"ebs-csi\") # disk storage for spilling\n",
    "    .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit\", \"100Gi\") # disk storage for spilling\n",
    "    .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path\", \"/data\") # disk storage for spilling\n",
    "    .config(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly\", \"false\") # disk storage for spilling\n",
    "    # The section with `spark.kubernetes.node.selector` is for specifying\n",
    "    # what nodes to use for the executor and in which Availability Zone (AZ)\n",
    "    # They need to be in the same zone\n",
    "    .config(\"spark.kubernetes.node.selector.topology.ebs.csi.aws.com/zone\", \"eu-central-1a\") # node selector\n",
    "    .config(\"spark.kubernetes.node.selector.plural.sh/scalingGroup\", \"xlarge-mem-optimized-on-demand\") # node selector\n",
    "    .config(\"spark.driver.host\", f\"sparknotebook-spark.{namespace}.svc.cluster.local\")\n",
    "    .config(\"spark.executor.instances\", \"2\") # number of Executors\n",
    "    .config(\"spark.executor.memory\", \"3g\") # Executor memory\n",
    "    .config(\"spark.executor.cores\", \"1\") # Executor cores \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e99825-77c7-407c-9157-a55f95576053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "\n",
    "schema = dg.SchemaParser.parseCreateTable(spark, \"\"\"\n",
    "    create table Test1 (\n",
    "    source string ,\n",
    "    language string ,\n",
    "    topic string ,\n",
    "    license string )\n",
    "\"\"\")\n",
    "\n",
    "data_rows = 4*10\n",
    "\n",
    "x3 = (dg.DataGenerator(sparkSession=spark, name=\"test_table_query\", rows=data_rows, partitions=20)\n",
    "      .withSchema(schema)\n",
    "      .withIdOutput()\n",
    "      .withColumnSpec(\"source\", values=[\"hackernews\", \"cc\", \"wikipedia\", \"academic\", \"books\", \"pubmed\", \"opensubtitiles\", \"youtubesubtitles\"], random=True)\n",
    "      .withColumnSpec(\"language\", values=[\"en\", \"de\", \"fr\", \"es\", \"ru\"], random=True)\n",
    "      .withColumnSpec(\"topic\", values=[\"software\", \"medical\", \"cultural\", \"academic\", \"hardware\", \"ai\", \"ml\", \"random\"], random=True)\n",
    "      .withColumnSpec(\"license\", values=[\"MIT\", \"GPL-v2\", \"GPL-v3\", \"private\", \"apache\", \"cc\"], random=True)\n",
    "     )\n",
    "\n",
    "x3_output_full = x3.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5679ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import BinaryType\n",
    "import zstd\n",
    "\n",
    "\n",
    "def compress(in_str: str) -> str:\n",
    "    return zstd.compress(in_str.encode(\"utf-8\"))\n",
    "    \n",
    "compress_udf = F.udf(compress, BinaryType())\n",
    "    \n",
    "(\n",
    "    x3_output_full\n",
    "    .withColumn(\"newcol\", compress_udf(F.col(\"topic\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b5d65-22cd-42b0-8fb2-1072b2bcb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.monotonic_ns()\n",
    "#x3_output_full.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_data\")\n",
    "x3_output_full.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_data2\", path='s3a://tims-delta-lake/delta-table-bench')\n",
    "print(\"Time elapsed : \", (time.monotonic_ns() - start)/10**9, \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
